\documentclass{beamer}

\usepackage{helvet}
\usepackage{hyperref, graphicx}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{multicol}
\usepackage{caption}

\usetheme[progressbar=frametitle, numbering=none]{metropolis}
\usecolortheme[snowy]{owl}
\setbeamertemplate{navigation symbols}{}
\AtBeginSection[ ]
{
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}
}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{11} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal - use in headings

% Custom colors
\usepackage{color}
\definecolor{TUGray}{RGB}{101,101,137}
\definecolor{TUBlack}{RGB}{0,0,10}
\definecolor{mygreen}{RGB}{45,111,63}
\definecolor{keywords}{RGB}{205,114,0}
\definecolor{comments}{RGB}{181,51,139}
\definecolor{strings}{RGB}{58,144,81}
\definecolor{numeric}{RGB}{66,110,176}
\definecolor{linos}{rgb}{0.4,0.4,0.4}
\definecolor{links}{rgb}{0,0.4,0.75}

\definecolor{bggray}{RGB}{232, 233, 235}

\setbeamercolor{alerted text}{fg=mygreen}
\setbeamercolor{block body alerted}{bg=bggray}
\setbeamercolor{normal text}{fg=TUBlack}\usebeamercolor*{normal text}

\setbeamercolor{codecol}{fg=TUGray!25!black,bg=bggray}

\hypersetup{colorlinks, linkcolor=links, urlcolor=links}

\usepackage[T1]{fontenc}
\usepackage[sfdefault,scaled=.85]{FiraSans}
\usepackage{newtxsf}

\usepackage{listings}

\usepackage{mathpazo}

\newtoggle{InString}{}% Keep track of if we are within a string
\togglefalse{InString}% Assume not initally in string

\newcommand\digitstyle{\color{numeric}}
\makeatletter
\newcommand{\ProcessDigit}[1]
{%
  \ifnum\lst@mode=\lst@Pmode\relax%
   {\digitstyle #1}%
  \else
    #1%
  \fi
}
\makeatother

\lstset{literate=%
    {0}{{{\ProcessDigit{0}}}}1
    {1}{{{\ProcessDigit{1}}}}1
    {2}{{{\ProcessDigit{2}}}}1
    {3}{{{\ProcessDigit{3}}}}1
    {4}{{{\ProcessDigit{4}}}}1
    {5}{{{\ProcessDigit{5}}}}1
    {6}{{{\ProcessDigit{6}}}}1
    {7}{{{\ProcessDigit{7}}}}1
    {8}{{{\ProcessDigit{8}}}}1
    {9}{{{\ProcessDigit{9}}}}1
	{<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
	% morestring=[b]",
    % morestring=[b]',
    % morecomment=[l]{//},
}

\lstdefinelanguage{Pseudo}{
    morekeywords={begin, end, return, while},
    morecomment=[l]{\#},
}

% Pseudocode style
\newcommand\pseudostyle{\lstset{
language=Pseudo,
basicstyle=\fontfamily{ccr}\scriptsize,
commentstyle=\it\scriptsize\color{linos},
keywordstyle=\it\bfseries\scriptsize,
mathescape=true,
literate=
    {=}{$\leftarrow{}$}{1}
    {==}{$={}$}{1},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=12pt,
belowskip=0pt,
frame=tB,
keepspaces=true
}}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttfamily\tiny,
numbers=left,
numberstyle=\tiny\color{linos},
morekeywords={self, np},              % Add keywords here
keywordstyle=\tiny\color{keywords},
commentstyle=\it\tiny\color{comments},    % Custom highlighting style
stringstyle=\tiny\color{strings},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=0pt,
belowskip=0pt,
escapeinside={(*@}{@*)},
frame=l,                         % Any extra options here
showstringspaces=false,
keepspaces=true
}}

% Pseudocode environment
\lstnewenvironment{pseudo}[1][]
{
    \pseudostyle
    \lstset{
        #1
    }
}
{}

% Python environment 
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{
	#1
	}
}
{}

% wrap the Python environment
\newenvironment{codeblock}
    {\hfill\begin{beamerboxesrounded}[lower=codecol, width=0.8\textwidth]
    \medskip

    }
    { 
    \end{beamerboxesrounded}\hfill
    }

\theoremstyle{example}
\newtheorem{question}{Question}

\newcommand{\ct}[1]{\lstinline[language=Python,basicstyle=\ttfamily\footnotesize,stringstyle=\small\color{strings}]!#1!}
\newcommand{\ttt}[1]{{\small\texttt{#1}}}
\newcommand{\lsitem}[2]{\ttt{{#1}[}\ct{#2}\ttt{]}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\cl}[1]{\mathcal{#1}}
\newcommand{\comment}[1]{}
\newcommand{\m}[1]{\begin{bmatrix}#1\end{bmatrix}}

\author{Chris Cornwell}
\date{September 11 and 16, 2025}
\title{Optimization from Calculus}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\section{Approximations from derivatives}

\begin{frame}{Linear approximations}
    $g:\bb R\to\bb R$, a twice-differentiable function (at least).  Write $w$ for input to $g$, so $g(w)$.

    \textbf{Linear approximation} to $g(w)$. At a point $(v, g(v))$ on its graph, function whose graph is the tangent line:
        \[h(w) = g(v) + g'(v)(w - v).\]
    \vspace*{-12pt}
    \begin{multicols}{2}
        \begin{itemize}
            \item Also called first order Taylor series approximation (or Taylor polynomial) of $g$. 
            \item Approximates values of $g$, for inputs near $v$.
        \end{itemize}

        \includegraphics[width=0.35\textwidth]{../../Images/graph-and-linearapproximation.png}
    \end{multicols}

    %\centering
    %\includegraphics
\end{frame}

\begin{frame}{Second order (quadratic) approximations}
    To approximate $g$ better (and in a larger interval around $v$), can use the second order Taylor polynomial. This is the function:
    \[h(w) = g(v) + g'(v)(w - v) + \frac12g''(v)(w - v)^2.\]

    \pause
    \begin{multicols}{2}
    {\footnotesize
    \begin{itemize}
        \item Incorporates both first and second derivative.  
        \pause
        \item {\bf Local minimum.} If $g'(v) = 0$ and $g''(v) > 0$, then $h(w) \ge g(v)$. (near $v$, approximation is good, values of $g(w) \geq g(v)$).
        \pause
        \item If $g'(v) = 0$ and $g''(v) < 0$, then opposite of the last item is true.
    \end{itemize}
    }

    \includegraphics[width=0.35\textwidth]{../../Images/graph-and-quadapproximations.png}
    \end{multicols}
\end{frame}

\begin{frame}{First order approximation in multiple variables}
    Extending to the setting of multiple variables. 
    \begin{itemize}
        \item Use the gradient where, if ${\bf w} = [w_1\ w_2\ \ldots\ w_N]^T$, then 
        \[\nabla g = [\frac{\partial g}{\partial w_1}\ \frac{\partial g}{\partial w_2}\ \ldots\ \frac{\partial g}{\partial w_N}]^T.\]
    \end{itemize}

    \pause
    {\bf Linear approximation:} is $h({\bf w}) = g({\bf v}) + \nabla g({\bf v})^T({\bf w} - {\bf v})$. 
    \begin{itemize}
        \pause
        \item \emph{Is} a (affine) linear function; graph is translation of subspace of $\bb R^{N+1}$ that is normal to $[-\nabla g({\bf v})^T,\ 1]^T$%, the translation is by a vector whose dot product with $[-\nabla g({\bf v})^T,\ 1]^T$ is equal to $g({\bf v}) - \nabla g({\bf v})^T{\bf v}$.)
        \pause
        \item The graph of $h({\bf w})$ is the tangent (hyper)plane to graph of $g({\bf w})$ at the point $({\bf v}, g({\bf v}))$.
    \end{itemize}
\end{frame}

\begin{frame}{Second order approximation in multiple variables}
    For second order approximations, use a matrix called the \textbf{Hessian}, $\nabla^2g$. \pause 
    \begin{itemize}
        \item The matrix $\nabla^2g$ (evaluated at ${\bf v}$) is the matrix of second order partial derivatives of $g$:
        \[\nabla^2g = \m{\frac{\partial^2 g}{\partial w_1\partial w_1} & \frac{\partial^2 g}{\partial w_1\partial w_2} & \ldots & \frac{\partial^2 g}{\partial w_1\partial w_N} \\ 
        \frac{\partial^2 g}{\partial w_2\partial w_1} & \frac{\partial^2 g}{\partial w_2\partial w_2} & \ldots & \frac{\partial^2 g}{\partial w_2\partial w_N} \\ 
         & & \ddots & \\ 
        \frac{\partial^2 g}{\partial w_N \partial w_1} & \frac{\partial^2 g}{\partial w_N\partial w_2} & \ldots & \frac{\partial^2 g}{\partial w_N\partial w_N}
        }.\]
    \end{itemize}

    \pause
    {\bf Second order approximation:} is 
        \[h({\bf w}) = g({\bf v}) + \nabla g({\bf v})^T({\bf w} - {\bf v}) + \frac12({\bf w}-{\bf v})^T\nabla^2g({\bf v})({\bf w} - {\bf v}). \]
    \vspace*{-12pt}
    \begin{itemize}
        \pause
        \item There is something like the second derivative test in this general case; is more involved to describe. In practice, techniques using first order approximations are the most commonly used.
    \end{itemize}
\end{frame}

\section{Stationary points}

\begin{frame}{Stationary points versus (local) minimal}
    A \textbf{stationary point} (or, \textbf{critical point}) of $g$ is a point ${\bf v}$ where $\nabla g({\bf v})$ is the zero vector. (Called the ``first order condition for optimality.'')
    \pause
    \begin{itemize}
        \item Means that ${\bf v}$ is point where all partial derivatives of $g$ are zero.
        \pause
        \item A minimum of $g$ can only occur at a stationary point. However, other things can happen at a stationary point too {--} maximum of $g$ or a ``saddle'' point.
    \end{itemize}
\end{frame}

\begin{frame}{The Chain rule}
    The \textbf{chain rule} is very useful for understanding derivatives and gradients. The more general version of it (``suped up'' from Calculus I):
    \pause
    \begin{alertblock}{Chain rule.}
        If the composition $f(g({\bf w}))$ is defined and each of $f$ and $g$ are differentiable, then the derivative of the composition is $Df(g({\bf w}))\cdot Dg({\bf w})$.\footnote{``$Df$'' and ``$Dg$'' mean, take the appropriate version of the derivative for the function. If the function is from $\mathbb R$ to $\mathbb R$, this is the $f'$ from Calc I; if it is multi-variable (from $\mathbb R^N$ to $\mathbb R$, for some $N>1$), then $Df$ means $\nabla f$. If $f$ has vector output, there is a matrix of partial derivatives for $Df$.} 
    \end{alertblock}
    
    \pause
    {\bf Example:} given a fixed vector ${\bf a}$, set $h({\bf w}) = \frac{1}{1+e^{{\bf a}^T{\bf w}}}$. This is a composition of $f(x) = \frac{1}{1+e^x}$ and $g({\bf w}) = {\bf a}^T{\bf w}$. Thus, 
    
    \[\scriptsize \nabla h({\bf w}) = f'(g({\bf w}))\cdot \nabla g({\bf w}) = \left(\frac{-e^{{\bf a}^T{\bf w}}}{1 + e^{{\bf a}^T{\bf w}}}\right){\bf a}.\]
\end{frame}

\begin{frame}{Example of stationary points}
    Here we look at the stationary points of the function $f(w_1,w_2) = w_1^3+3w_1w_2+w_2^3$.

    {\small
    \begin{itemize}
        \item $\nabla f(w_1,w_2) = [3w_1^2 + 3w_2,\quad 3w_1+3w_2^2]^T$
    \end{itemize}
    \pause
    \begin{multicols}{2}
        \begin{itemize}
            \item If a stationary pt., $3w_1^2 + 3w_2=0$ and $3w_1+3w_2^2=0$; so, $w_2 = -w_1^2$; using this and simplifying, $w_1+w_1^4 = 0$.
            \pause
            \item We get: either $w_1=0$ (with $w_2=0$ also), or $w_1=-1$ (with $w_2=-1$ also). \newline 
            See Figure on right.
        \end{itemize}

        \begin{figure}
        \includegraphics[width=0.3\textwidth]{../../Images/stationary-points_PolyFunction-2Vars.png}
        \captionsetup{font=scriptsize}
        \caption*{Vertical lines placed over stationary\newline\ points $(0,0)$ and $(-1,-1)$}
        \end{figure}
    \end{multicols}
    }
\end{frame}

\begin{frame}{Convex functions}
    Function $g:\mathbb R\to\mathbb R$ is called \textbf{convex} if $g''(v) \ge 0$ for all $v$.\footnote{A different definition (which is equivalent): $g$ is \textbf{convex at $v$} if the linear approximation at $v$ has a graph that is below the graph of $g$, near $v$; it is then \textbf{convex} if it is convex at $v$ for all $v$ in $\mathbb R$. This extends to multiple variables.} 

    \pause
    \begin{alertblock}{Example.} The function $g(w) = e^w - w$ is a convex function, since $g''(w) = e^w$, and $e^v$ is bigger than zero for all $v$.
    \end{alertblock}
    \centering 
    \includegraphics[width=0.35\textwidth]{../../Images/convex-1Var.png}
    \vfill
\end{frame}

\end{document}