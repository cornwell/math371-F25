\frametitle{Implementing Gradient Descent}
    Some notes on implementation of the gradient descent updates.

    {\color{mygreen}1.} Each partial deriv.\ of $g_{\mathcal S}$, can compute it in one line of code. \pause Presuming \texttt{x}, \texttt{y} are arrays of coordinates for data, current parameter values are \texttt{b}, \texttt{w}, the following gives partial derivatives:

\begin{codeblock}

\begin{python}
partial_w = 2*np.sum( (w*x + b - y)*x )
partial_b = 2*np.sum( (w*x + b - y) )
\end{python}

\end{codeblock}

    \pause
    {\color{mygreen}2.} To implement GD, want more than one function {--} at the least, one to compute the gradient (given current parameters); another that performs update and checks for stopping. \textit{Roughly}\ldots
\pause

\begin{pseudo}
## lr is learning rate; threshhold is for stopping;
input: X, y, lr, threshhold
params = initial array of parameters
while (max of last_update > threshhold){
    grad = compute_grad(params, X, y)
    last_update = | grad / params | ## entrywise array division
    # handle params[i] near 0
    params = params - lr*grad
}
return params
\end{pseudo}
