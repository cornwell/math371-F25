\documentclass{beamer}

\usepackage{helvet}
\usepackage{hyperref, graphicx}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{multicol}
\usepackage{caption}

\usetheme[progressbar=frametitle, numbering=none]{metropolis}
\usecolortheme[snowy]{owl}
\setbeamertemplate{navigation symbols}{}
\AtBeginSection[ ]
{
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}
}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{11} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal - use in headings

% Custom colors
\usepackage{color}
\definecolor{TUGray}{RGB}{101,101,137}
\definecolor{TUBlack}{RGB}{0,0,10}
\definecolor{mygreen}{RGB}{45,111,63}
\definecolor{keywords}{RGB}{205,114,0}
\definecolor{comments}{RGB}{181,51,139}
\definecolor{strings}{RGB}{58,144,81}
\definecolor{numeric}{RGB}{66,110,176}
\definecolor{linos}{rgb}{0.4,0.4,0.4}
\definecolor{links}{rgb}{0,0.4,0.75}

\definecolor{bggray}{RGB}{232, 233, 235}

\setbeamercolor{alerted text}{fg=mygreen}
\setbeamercolor{normal text}{fg=TUBlack}\usebeamercolor*{normal text}

\setbeamercolor{codecol}{fg=TUGray!25!black,bg=bggray}

\hypersetup{colorlinks, linkcolor=links, urlcolor=links}

\usepackage[T1]{fontenc}
\usepackage[sfdefault,scaled=.85]{FiraSans}
%\usepackage{newtxsf}
\usepackage{mathpazo}

\usepackage{listings}

\newtoggle{InString}{}% Keep track of if we are within a string
\togglefalse{InString}% Assume not initally in string

\newcommand\digitstyle{\color{numeric}}
\makeatletter
\newcommand{\ProcessDigit}[1]
{%
  \ifnum\lst@mode=\lst@Pmode\relax%
   {\digitstyle #1}%
  \else
    #1%
  \fi
}
\makeatother

\lstset{literate=%
    {0}{{{\ProcessDigit{0}}}}1
    {1}{{{\ProcessDigit{1}}}}1
    {2}{{{\ProcessDigit{2}}}}1
    {3}{{{\ProcessDigit{3}}}}1
    {4}{{{\ProcessDigit{4}}}}1
    {5}{{{\ProcessDigit{5}}}}1
    {6}{{{\ProcessDigit{6}}}}1
    {7}{{{\ProcessDigit{7}}}}1
    {8}{{{\ProcessDigit{8}}}}1
    {9}{{{\ProcessDigit{9}}}}1
	{<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
	% morestring=[b]",
    % morestring=[b]',
    % morecomment=[l]{//},
}

\lstdefinelanguage{Pseudo}{
    morekeywords={begin, end, return, while},
    morecomment=[l]{\#},
}

% Pseudocode style
\newcommand\pseudostyle{\lstset{
language=Pseudo,
basicstyle=\fontfamily{ccr}\scriptsize,
commentstyle=\it\scriptsize\color{linos},
keywordstyle=\it\bfseries\scriptsize,
mathescape=true,
literate=
    {=}{$\leftarrow{}$}{1}
    {==}{$={}$}{1},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=12pt,
belowskip=0pt,
frame=tB,
keepspaces=true
}}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttfamily\tiny,
numbers=left,
numberstyle=\tiny\color{linos},
morekeywords={self, np},              % Add keywords here
keywordstyle=\tiny\color{keywords},
commentstyle=\it\tiny\color{comments},    % Custom highlighting style
stringstyle=\tiny\color{strings},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=0pt,
belowskip=0pt,
escapeinside={(*@}{@*)},
frame=l,                         % Any extra options here
showstringspaces=false,
keepspaces=true
}}

% Pseudocode environment
\lstnewenvironment{pseudo}[1][]
{
    \pseudostyle
    \lstset{
        #1
    }
}
{}

% Python environment 
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{
	#1
	}
}
{}

% wrap the Python environment
\newenvironment{codeblock}
    {\hfill\begin{beamerboxesrounded}[lower=codecol, width=0.8\textwidth]
    \medskip

    }
    { 
    \end{beamerboxesrounded}\hfill
    }

\theoremstyle{example}
\newtheorem{question}{Question}

\newcommand{\ct}[1]{\lstinline[language=Python,basicstyle=\ttfamily\footnotesize,stringstyle=\small\color{strings}]!#1!}
\newcommand{\ttt}[1]{{\small\texttt{#1}}}
\newcommand{\lsitem}[2]{\ttt{{#1}[}\ct{#2}\ttt{]}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\cl}[1]{\mathcal{#1}}
\newcommand{\comment}[1]{}

\author{Chris Cornwell}
\date{September 11, 2025}
\title{Linear regression through Optimization}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\section{Generalizing Linear regression to multiple variables}

\begin{frame}{Extending the linear regression procedure}
    If you have data $({\bf x}_1,y_1), ({\bf x}_2,y_2), \ldots, ({\bf x}_P, y_P)$ where, for some $N>1$ the input ${\bf x}_i$ is a vector in $\bb R^N$, can you still do linear regression?

    \pause
    For each $i$ with $1\le i\le P$, say that ${\bf x}_i = {\scriptsize \begin{bmatrix}x_{1,i}\\ x_{2,i}\\ \vdots\\ x_{N,i}\end{bmatrix}}$. Define a $P\times(N+1)$ matrix $A$ so that the $i^{th}$ row is $[1, x_{1,i}, x_{2,i}, \ldots, x_{N,i}]$.\pause \newline 
    In other words, set $\tilde{\bf x}_i$ equal to the vector in $\bb R^{N+1}$ with $1$ as its first component and ${\bf x}_i$ for the remaining $N$ components. Then 
        \[A = \begin{bmatrix} \overset{\rule{0.4cm}{0.02cm}}{\phantom{.}} & \tilde{\bf x}_1^T & \overset{\rule{0.4cm}{0.02cm}}{\phantom{.}} \\ \overset{\rule{0.4cm}{0.02cm}}{\phantom{.}} & \tilde{\bf x}_2^T & \overset{\rule{0.4cm}{0.02cm}}{\phantom{.}} \\ & \vdots & \\ \overset{\rule{0.4cm}{0.02cm}}{\phantom{.}} & \tilde{\bf x}_P^T & \overset{\rule{0.4cm}{0.02cm}}{\phantom{.}} \end{bmatrix}.\]
\end{frame}

\begin{frame}{Extending the linear regression procedure}
    A solution $\tilde{\bf w}^\star = [b^\star, w_1^\star, \ldots, w_N^\star]^T$ to the normal equations $A^TA\tilde{\bf w} = A^T{\bf y}$ (with $A$ as above) gives coefficients for a linear model for the data.\footnote{\small The matrix $A^TA$ is $(N+1)\times(N+1)$ and $A^T{\bf y}$ is a vector in $\bb R^{N+1}$.}

    \vspace*{-2pt}
    \pause
    Write ${\bf w}^\star \in \bb R^{N}$ for the non-constant coefficient vector ${\scriptsize [w_1^\star,w_2^\star,\ldots,w_N^\star]^T}$. The affine linear model on the data, in $N$ variables, is \vspace*{-4pt} 
    \[f_{\tilde{\bf w}^\star}({\bf x}) = b^\star + w_1^\star x_1 + \ldots + w_N^\star x_N = b^\star + {\bf x}^T{\bf w}^\star.\] 

    \vspace*{-15pt}   
    \begin{enumerate}
        \pause
        \item[(1)] $f_{\tilde{\bf w}^\star}({\bf x})$ is affine linear, meaning the difference in two function values is a dot product on the difference of the input. Specifically, $f_{\tilde{\bf w}^\star}({\bf x}) - f_{\tilde{\bf w}^\star}({\bf x}') = ({\bf x} - {\bf x}')^T{\bf w}^\star$.
        \pause
        \item[(2)] The graph of $f_{\tilde{\bf w}^\star}({\bf x})$ is a hyperplane\footnote{\small Generalization of a plane in $\bb R^3$.} in $\bb R^{N+1}$ with normal vector ${\scriptsize [1,-w_1^\star, \ldots, -w_N^\star]^T}$.
    \end{enumerate}
    \vfill
\end{frame}

\begin{frame}{Example}
    \textbf{A concrete example:} using the data in \ct{'Advertising.csv'}, found in the \href{https://github.com/cornwell/math371-S25/blob/main/DataSets/}{DataSets folder} of the course site. \newline 
    This contains data on amounts spent (in thousands of dollars) on TV, Radio, and Newspaper advertising in 200 different markets, as well as the amounts sold in each market (in thousands of units).
    
    \pause
    \begin{multicols}{2}
    To perform linear regression, with independent variables \ct{'TV'}, \ct{'Radio'}, and \ct{'Newspaper'} and setting $y$ equal to \ct{'Sales'}, one may follow the procedure above. The matrix $A$ described is $200\times 4$ ($N=3$).\pause\ The resulting coefficients are (approximately) \[{\scriptsize [b^\star, w_1^\star, w_2^\star, w_3^\star] = [2.9389, 0.0458, 0.1885, -0.001].}\]
    
    \begin{figure}[tight]
    \includegraphics[width=0.3\textwidth]{../../Images/multiregression-higherangle.png}
    \caption*{{\footnotesize 3D projection of\\ Advertising model}}
    \end{figure}
    \end{multicols}
\end{frame}

\section{Measuring the fitness of linear models}

\begin{frame}{``Noise'' and linear regression model}
    \textbf{Underlying assumption:} the (input) variables $x_1,\ldots,x_N$ are not random, but there is natural variability in the ``$y$-direction'' (value of $f_{\tilde{\bf w}}$), represented with a random variable.\footnote{Note that, if you instead consider the random variable to be in another direction (swapping some $x_j$ with $y$) then there is a \emph{different} linear regression.} \newline 
    \pause
    So, there is a $b$ and ${\bf w} = [w_1,\ldots, w_N]^T$, so that the relationship between ${\bf x}$ and $y$ is 
    \[y = b + {\bf x}^T{\bf w} + \varepsilon,\]
    where $\varepsilon$ is a random variable with mean (first moment) equal to $0$. 
    \pause
    \begin{itemize}
        \item Critically, the distribution of $\varepsilon$ should be independent of ${\bf x}$. 
        \pause
        \item A common consideration: $\varepsilon$ \href{https://en.wikipedia.org/wiki/Normal_distribution}{normally distributed}.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Least Squares loss function}
    On given data $({\bf x}_1, y_1), ({\bf x}_2, y_2), \ldots, ({\bf x}_P, y_P)$, a common measure for how well a linear regression model fits is the \textbf{Mean Squared Error}. If the intercept and ${\bf w}$ vector for the regression line are $b^\star$ and ${\bf w}^\star$ then we have $\hat{y}_i = b^\star + {\bf x}_i^T{\bf w}^\star$ and the Mean Squared Error is 
        \[\frac1{P}\sum_{i=1}^P\left(\hat{y}_i - y_i\right)^2 = \frac1{P}\sum_{i=1}^P\left(b^\star + {\bf x}_i^T{\bf w}^\star - y_i\right)^2.\]
    
    \pause
    It is helpful to think of this as a function on an arbitrary choice of parameters $\tilde{\bf w} = (b, {\bf w})$.  Dropping the division by $P$, this is the \textbf{Least Squares loss} function: \pause\ if $f_{\tilde{\bf w}}({\bf x}) = b + {\bf x}^T{\bf w}$ is the linear model for parameters $\tilde{\bf w}=(b,w_1,\ldots,w_N)$ then this loss function is  
    
        \[g(\tilde{\bf w}) = \sum_{i=1}^P\left(f_{\tilde{\bf w}}({\bf x}_i) - y_i\right)^2.\]
    
\end{frame}

\begin{frame}
    \frametitle{Minimizing the Least Squares loss}
    For fixed data $\{({\bf x}_i, y_i)\}_{i=1}^P$, how do we solve the problem 
                \[\underset{\tilde{\bf w}}{\text{minimize}}\ g(\tilde{\bf w})?\]
    \pause
    We can use methods from calculus to determine the minimum (and the minimizer, to set equal to $\tilde{\bf w}$). Specifically, the first order condition is useful {--} that we want a value of $\tilde{\bf w}$ where all partial derivatives of $g(\tilde{\bf w})$ are equal to zero. 
    \begin{itemize}
        \pause
        \item Note, what are the variables for the partial derivative? They are the parameters in $\tilde{\bf w}$, namely $b,w_1,w_2,\ldots,w_N$.
        \pause
        \item There will be the input (independent variables) ${\bf x}$ to the linear function; but $g$ is a function of the \emph{parameters}.
    \end{itemize}
    \vfill
    \pause
    Next: a review of calculus minimization techniques.
\end{frame}

\begin{frame}
    \frametitle{Meaning of Least Squares loss}
    For $1\le i\le P$, the quantity $|f_{b,{\bf w}}({\bf x}_i) - y_i|$ is the vertical distance from $({\bf x}_i, y_i)$ to the point predicted by the linear model, $({\bf x}_i, f_{\tilde{\bf w}}({\bf x}_i))$. 
    
    \pause
    \begin{center} 
    \includegraphics[width=0.3\textwidth]{../../Images/lsr-error-lines.png}
    \end{center}

    \pause
    The length of the vector ${\bf y} - \hat{\bf y}$ (\textit{i.e.}, the distance from ${\bf y}$ to the vector of predictions, in the column space of our feature matrix) is equal to 
        \[\sqrt{(f_{\tilde{\bf w}}({\bf x}_1) - y_1)^2 + (f_{\tilde{\bf w}}({\bf x}_2) - y_2)^2 + \ldots + (f_{\tilde{\bf w}}({\bf x}_P) - y_P)^2} = \sqrt{g(\tilde{\bf w})}.\]

    \pause
    We see that minimizing $g(\tilde{\bf w})$ is the same as minimizing that distance, giving the $\hat{\bf y}$ in the column space that makes ${\bf y} - \hat{\bf y}$ orthogonal to the column space.
\end{frame}

\end{document}