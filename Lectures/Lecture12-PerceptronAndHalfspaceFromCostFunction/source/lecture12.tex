\documentclass{beamer}

\usepackage{helvet}
\usepackage{hyperref, graphicx}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{ulem}

\usetheme{default}
\setbeamertemplate{navigation symbols}{}
\AtBeginSection[ ]
{
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}
}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{11} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal - use in headings

% Custom colors
\usepackage{color}
\definecolor{TUGray}{RGB}{101,101,137}
\definecolor{TUBlack}{RGB}{30,0,0}
\definecolor{mygreen}{RGB}{45,111,63}
\definecolor{keywords}{RGB}{205,114,0}
\definecolor{comments}{RGB}{181,51,139}
\definecolor{strings}{RGB}{58,144,81}
\definecolor{numeric}{RGB}{66,110,176}
\definecolor{linos}{rgb}{0.4,0.4,0.4}
\definecolor{links}{rgb}{0,0.4,0.75}

\definecolor{bggray}{RGB}{232, 233, 235}

\usecolortheme[named=mygreen]{structure}
\setbeamercolor{normal text}{fg=TUBlack}\usebeamercolor*{normal text}

\setbeamercolor{codecol}{fg=TUGray!25!black,bg=bggray}

\hypersetup{colorlinks, linkcolor=links, urlcolor=links}

\usepackage[T1]{fontenc}
\usepackage[sfdefault,scaled=.85]{FiraSans}
\usepackage{mathpazo}

\usepackage{listings}

\newtoggle{InString}{}% Keep track of if we are within a string
\togglefalse{InString}% Assume not initally in string

\newcommand\digitstyle{\color{numeric}}
\makeatletter
\newcommand{\ProcessDigit}[1]
{%
  \ifnum\lst@mode=\lst@Pmode\relax%
   {\digitstyle #1}%
  \else
    #1%
  \fi
}
\makeatother

\lstset{literate=%
    {0}{{{\ProcessDigit{0}}}}1
    {1}{{{\ProcessDigit{1}}}}1
    {2}{{{\ProcessDigit{2}}}}1
    {3}{{{\ProcessDigit{3}}}}1
    {4}{{{\ProcessDigit{4}}}}1
    {5}{{{\ProcessDigit{5}}}}1
    {6}{{{\ProcessDigit{6}}}}1
    {7}{{{\ProcessDigit{7}}}}1
    {8}{{{\ProcessDigit{8}}}}1
    {9}{{{\ProcessDigit{9}}}}1
	{<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
	% morestring=[b]",
    % morestring=[b]',
    % morecomment=[l]{//},
}

\lstdefinelanguage{Pseudo}{
    morekeywords={return, while, if, for, input},
    morecomment=[l]{\#},
}

% Pseudocode style
\newcommand\pseudostyle{\lstset{
language=Pseudo,
basicstyle=\fontfamily{ccr}\scriptsize,
commentstyle=\it\scriptsize\color{linos},
keywordstyle=\it\bfseries\scriptsize,
mathescape=true,
literate=
    {=}{$\leftarrow{}$}{1}
    {==}{$={}$}{1}
    {<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=12pt,
belowskip=0pt,
frame=tB,
keepspaces=true
}}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttfamily\tiny,
numbers=left,
numberstyle=\tiny\color{linos},
morekeywords={self, np},              % Add keywords here
keywordstyle=\tiny\color{keywords},
commentstyle=\it\tiny\color{comments},    % Custom highlighting style
stringstyle=\tiny\color{strings},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=0pt,
belowskip=0pt,
escapeinside={(*@}{@*)},
frame=l,                         % Any extra options here
showstringspaces=false,
keepspaces=true
}}

% Pseudocode environment
\lstnewenvironment{pseudo}[1][]
{
    \pseudostyle
    \lstset{
        #1
    }
}
{}

% Python environment 
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{
	#1
	}
}
{}

% wrap the Python environment
\newenvironment{codeblock}
    {\hfill\begin{beamerboxesrounded}[lower=codecol, width=0.8\textwidth]
    \medskip

    }
    { 
    \end{beamerboxesrounded}\hfill
    }

\theoremstyle{example}
\newtheorem{question}{Question}

\newcommand{\ct}[1]{\lstinline[language=Python,basicstyle=\ttfamily\footnotesize,stringstyle=\small\color{strings}]!#1!}
\newcommand{\ttt}[1]{{\small\texttt{#1}}}
\newcommand{\lsitem}[2]{\ttt{{#1}[}\ct{#2}\ttt{]}}

\author{Chris Cornwell}
\date{November 3, 2025}
\title{Cost Functions for Half-space Models}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\section{The Perceptron Cost and Softmax}

\section{The Margin Perceptron and Other Cost Functions}
%%%%
\begin{frame}
    \frametitle{Returning to the ``Simple Example'' for Perceptron algorithm}
    Example in $\mathbb R^2$, with $P=4$ points.

    \begin{center}
    \ttt{x}: ${\scriptsize\begin{bmatrix}-1 & 3 \\ -1 & -1 \\ 3 & -1 \\ 0 & 1.5\end{bmatrix}}$  \qquad\qquad
    \ttt{y}: ${\scriptsize\begin{bmatrix}-1 \\ -1 \\ 1 \\ 1\end{bmatrix}}$
    \end{center}
    Final $\tilde{\bf w} = {\scriptsize \begin{bmatrix}1\\ 4\\-0.5\end{bmatrix} }$; hyperplane in $\mathbb R^2$ (a line) shown below.
    \centering
    \includegraphics[height=0.3\textheight]{../../Images/ex2_data_halfspaceH.png}

    What if this data were part of a sample, which has noise? What about the point near the hyperplane?
\end{frame}

%%%%
\begin{frame}
    \frametitle{Getting a Buffer {--} a ``margin''}
    Say that $S = \{{\bf x}_i, y_i\}_{i=1}^P$ (with $y_i=\pm1$) is linearly separable. Instead of simply trying to find a hyperplane that successfully separates the data, try to find one that has some distance from points on each side.

    \pause 
    Can express this as wanting no points between two parallel hyperplanes {--} between the set of ${\bf x}$ where $\tilde{\bf x}^\top\tilde{\bf w} = 1$ and where $\tilde{\bf x}^\top\tilde{\bf w} = -1$.
    
    \begin{multicols}{2}
    \begin{figure} 
    \includegraphics[width=0.35\textwidth]{../../Images/marginHalfspace.png}
    \caption{Taken from Figure 4.4 in textbook, p.~79. (Note: they used blue for the -1 label.)}
    \end{figure}

    \vspace*{12pt} \phantom{i}\newline 
    This is equivalent to wanting, for all $i=1,\ldots,P$, $\tilde{\bf x}_i^\top\tilde{\bf w} \ge 1$ if $y_i=1$ and $\tilde{\bf x}_i^\top\tilde{\bf w} \le 1$ if $y_i=-1$. That is, we want $y_i(\tilde{\bf x}_i^\top\tilde{\bf w}) \ge 1$. \newline 
    Recall, $S$ being linearly separable implies that there must be a $\tilde{\bf w}$ that achieves this.
    \end{multicols}
\end{frame}
\begin{frame}
    \frametitle{Cost function for margin halfspace model}
    As we are looking for $\tilde{\bf w}$ so that $y_i(\tilde{\bf x}_i^\top\tilde{\bf w}) \ge 1$ for all $i=1,\ldots,P$, it makes sense to have $({\bf x}_i, y_i)$ contribute $0$ to our cost function if this inequality works for that $i$. So, we might set 
            \[g_3(\tilde{\bf w}) = \sum_{i=1}^P\max\left(0, 1 - y_i(\tilde{\bf x}_i^\top\tilde{\bf w})\right).\]
    
    (Note that, when not using a margin and using the max cost function, rather than softmax, there was a trivial minimizer of $\tilde {\bf w} = \vec{\bf 0}$, which we do not want to use.)

    \textbf{Softmax approximation} for margin halfspace: As before, if we want a smooth function we can replace the $\max$ in the cost function with $\operatorname{softmax}$: 
        \[(Alternate)\quad g_3(\tilde{\bf w}) = \sum_{i=1}^P\operatorname{softmax}\left(0, 1 - y_i(\tilde{\bf x}_i^\top\tilde{\bf w})\right).\]

    \textbf{Squared max} for margin halfspace: Another option for a 
    \[g_4(\tilde{\bf w}) = \sum_{i=1}^P\left(\max\left(0, 1 - y_i(\tilde{\bf x}_i^\top\tilde{\bf w})\right)\right)^2.\]
\end{frame}
\section{Accuracy and Counting Costs}

\begin{frame}{The Counting Cost Function}
    Suppose that we have found our stationary point (or approximation to it), $\tilde{\bf w}^{\star} = (b^\star, {\bf w}^\star)$. Given a ``new'' data point, ${\bf x}_{new}$, the halfspace model that we would use to predict labels on data is: make predicted $y_{new} = 1$ if the dot product $\tilde{{\bf x}_{new}}\cdot\tilde{\bf w}^\star = b^\star + {\bf x}_{new}\cdot{\bf w}^\star$ is positive; alternatively, when that dot product is negative, predict $y_{new} = -1$.

    Using the ``sign'' function: 
        \[f({\bf x}_{new}) = \operatorname{sign}\left(\tilde{\bf x}_{new}\cdot\tilde{\bf w}^\star\right).\]
    
    To measure the accuracy of this model on our given data $S = \{({\bf x}_i, y_i)\}$, compute how many this prediction function gets to agree with $y_i$; that is, 
        \[g_0(\tilde{\bf w}^\star) = \sum_{i=1}^P\max\left(0, -y_i f({\bf x}_i)\right) = \sum_{i=1}^P\max\left(0, -y_i\ \operatorname{sign}(\tilde{\bf x}_{new}\cdot\tilde{\bf w}^\star)\right).\]
    The accuracy is then $1 - \frac{g_0({\bf w}^\star)}{P}$. We did not directly minimize this function {--} there are several problems with using it in an optimization procedure. Instead we optimized one of the previous cost functions; but they are related. 
\end{frame}
\begin{frame}{Smoothing the Counting Cost}
    Logistic function 

    the hyperbolic tangent... more like $\operatorname{tanh}(y_i(\tilde{\bf x}\cdot\tilde{\bf w})/2)$; want this approximately equal to 1. Then...terms in $g_2$ will be approximately 0.
\end{frame}
\section{The Logistic Function as your Cost}
\begin{frame}{Negative Log-Loss}
    Use $\sigma((b+x_i\cdot w))$ as the output of the model; labels 1 and 0 instead; ...
\end{frame}

\end{document}