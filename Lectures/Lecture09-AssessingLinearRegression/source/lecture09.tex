\documentclass{beamer}

\usepackage{helvet}
\usepackage{hyperref, graphicx}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{multicol}

%\graphicspath{{../../}}

\usetheme{default}
\setbeamertemplate{navigation symbols}{}
\AtBeginSection[ ]
{
\begin{frame}{Outline}
    \tableofcontents[currentsection]
\end{frame}
}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{11} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal - use in headings

% Custom colors
\usepackage{color}
\definecolor{TUGray}{RGB}{101,101,137}
\definecolor{TUBlack}{RGB}{30,0,0}
\definecolor{mygreen}{RGB}{45,111,63}
\definecolor{keywords}{RGB}{205,114,0}
\definecolor{comments}{RGB}{181,51,139}
\definecolor{strings}{RGB}{58,144,81}
\definecolor{numeric}{RGB}{66,110,176}
\definecolor{linos}{rgb}{0.4,0.4,0.4}
\definecolor{links}{rgb}{0,0.4,0.75}

\definecolor{bggray}{RGB}{232, 233, 235}

\usecolortheme[named=mygreen]{structure}
\setbeamercolor{normal text}{fg=TUBlack}\usebeamercolor*{normal text}

\setbeamercolor{codecol}{fg=TUGray!25!black,bg=bggray}

\hypersetup{colorlinks, linkcolor=links, urlcolor=links}

\usepackage[T1]{fontenc}
\usepackage[sfdefault,scaled=.85]{FiraSans}
\usepackage{mathpazo}

\usepackage{listings}

\newtoggle{InString}{}% Keep track of if we are within a string
\togglefalse{InString}% Assume not initally in string

\newcommand\digitstyle{\color{numeric}}
\makeatletter
\newcommand{\ProcessDigit}[1]
{%
  \ifnum\lst@mode=\lst@Pmode\relax%
   {\digitstyle #1}%
  \else
    #1%
  \fi
}
\makeatother

\lstset{literate=%
    {0}{{{\ProcessDigit{0}}}}1
    {1}{{{\ProcessDigit{1}}}}1
    {2}{{{\ProcessDigit{2}}}}1
    {3}{{{\ProcessDigit{3}}}}1
    {4}{{{\ProcessDigit{4}}}}1
    {5}{{{\ProcessDigit{5}}}}1
    {6}{{{\ProcessDigit{6}}}}1
    {7}{{{\ProcessDigit{7}}}}1
    {8}{{{\ProcessDigit{8}}}}1
    {9}{{{\ProcessDigit{9}}}}1
	{<=}{{\(\leq\)}}1
	{>=}{{\(\geq\)}}1,
	% morestring=[b]",
    % morestring=[b]',
    % morecomment=[l]{//},
}

\lstdefinelanguage{Pseudo}{
    morekeywords={begin, end, return, while},
    morecomment=[l]{\#},
}

% Pseudocode style
\newcommand\pseudostyle{\lstset{
language=Pseudo,
basicstyle=\fontfamily{ccr}\scriptsize,
commentstyle=\it\scriptsize\color{linos},
keywordstyle=\it\bfseries\scriptsize,
mathescape=true,
literate=
    {=}{$\leftarrow{}$}{1}
    {==}{$={}$}{1},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=12pt,
belowskip=0pt,
frame=tB,
keepspaces=true
}}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttfamily\tiny,
numbers=left,
numberstyle=\tiny\color{linos},
morekeywords={self, np},              % Add keywords here
keywordstyle=\tiny\color{keywords},
commentstyle=\it\tiny\color{comments},    % Custom highlighting style
stringstyle=\tiny\color{strings},
xleftmargin=18pt,
xrightmargin=4pt,
aboveskip=0pt,
belowskip=0pt,
escapeinside={(*@}{@*)},
frame=l,                         % Any extra options here
showstringspaces=false,
keepspaces=true
}}

% Pseudocode environment
\lstnewenvironment{pseudo}[1][]
{
    \pseudostyle
    \lstset{
        #1
    }
}
{}

% Python environment 
\lstnewenvironment{python}[1][]
{
	\pythonstyle
	\lstset{
	#1
	}
}
{}

% wrap the Python environment
\newenvironment{codeblock}
    {\hfill\begin{beamerboxesrounded}[lower=codecol, width=0.8\textwidth]
    \medskip

    }
    { 
    \end{beamerboxesrounded}\hfill
    }

\theoremstyle{example}
\newtheorem{question}{Question}

\newcommand{\ct}[1]{\lstinline[language=Python,basicstyle=\ttfamily\footnotesize,stringstyle=\small\color{strings}]!#1!}
\newcommand{\ttt}[1]{{\small\texttt{#1}}}
\newcommand{\lsitem}[2]{\ttt{{#1}[}\ct{#2}\ttt{]}}

\author{Chris Cornwell}
\date{Oct 9, 2025}
\title{Assessing accuracy of linear regression}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\section{Confidence intervals with linear regression}

%%%%
\begin{frame}
\frametitle{Difference between parameters from ``population'' vs. from data}
\begin{itemize}
    \item Modeling relationship between independent variables and $y$ with a linear model (with noise in the $y$-coordinate direction). In other words, the modeled relationship is that
        \[y \approx b + {\bf x}^T{\bf w}.\]
    for some parameters ${\bf w}$, $b$. 
    \pause
    \item Alternatively, we may have some function\footnote{There is some number $M$ so that $f:\mathbb R^N\to\mathbb R^M$.} of the variables $f({\bf x})$ and a modeled relationship 
        \[y \approx b + f({\bf x})^T{\bf w}.\]
\end{itemize}

\pause
However, we find values for ${\bf w}$ and $b$ by using observed data, \emph{sampled} from a ``population.''  Among the entire population, there is a best fit linear model having some parameters. However, from the observed data $({\bf x}_1,y_1), ({\bf x}_2,y_2), \ldots, ({\bf x}_P,y_P)$ if our procedure determines best fit parameters $b^\star$ and ${\bf w}^\star$, these are not (necessarily) the parameters for the population linear model.

\end{frame}

%%%%
\begin{frame}[fragile]
\frametitle{Example}
\textbf{Simulate noisy linear data:} make 30 points, from a line with slope $-1.6$ and $y$-intercept $0.8$; put noise into the $y$-coordinate with standard deviation $\sigma = 0.5$. 
\pause

\begin{codeblock}

\begin{python}
x = np.random.uniform(0, 2, size=30)

def simulate_data(x, std):
    return -1.6*x + 0.8 + np.random.normal(0, std, size=len(x))
y = simulate_data(x, 0.5)
\end{python}

\end{codeblock}

\pause
With this simulated data set, compute the slope $w^\star$ and intercept $b^\star$ from linear regression; then put $w^\star$, and $b^\star$, into a list of slopes, intercepts respectively. Iterate this 1000 times $\to$ a list of 1000 slopes, another with 1000 intercepts. 
\vspace*{12pt}

\pause
What is the mean of these slopes and intercepts that were found?

\end{frame}

%%%%
\begin{frame}
\frametitle{Sample statistic, relation to population statistic}
This is fundamental to statistics. 
\begin{itemize}
    \item Say that a sample of 2000 people are selected from around the country and their height is measured. Mean of these 2000 heights: sample mean.
    \pause
    \item Sample mean differs from the true mean height of the entire population of the country. (Not by much, perhaps.)
    \begin{itemize}
        \item Weak Law of Large Numbers: if $s$ random samples of 2000 people taken, and each sample mean calculated then, as $s\to\infty$, the average of those $s$ sample means limits (in probability) to the population mean.
    \end{itemize}
    \pause
    \item Analogous thing happens with data from linear relationship with noise {--} think of parameters $w^\star$ and $b^\star$ as sample statistics (like the sample mean).
\end{itemize}

\end{frame}

%%%%
\begin{frame}
\frametitle{Confidence intervals}
How close do we suspect $w^\star$ and $b^\star$ to be to the \emph{true} (population) slope and intercept?

\pause
\textbf{Standard Error (SE):}  Suppose that for our error term $\varepsilon$, we have $\operatorname{Var}(\varepsilon) = \sigma^2$. Sample size: $P$.

\pause
Using $\bar{x}$ for the average of $x_1,\ldots,x_P$,

\[(SE(w^\star))^2 = \frac{\sigma^2}{\sum_{i=1}^P(x_i - \bar{x})^2};\]
\[(SE(b^\star))^2 = \sigma^2\left(\frac1{P} + \frac{\bar{x}^2}{\sum_{i=1}^P(x_i - \bar{x})^2}\right).\]

\pause
\emph{Roughly}, the Standard Error is the amount, on average, that $w^\star$ (resp.\ $b^\star$) differs from true slope $w$ (resp.\ true intercept $b$).\footnote{These formulae are just for single-variable linear regression.}

\pause
$\sigma$ is unknown, but can estimate it with \textbf{residual standard error}:
    \[\hat{\sigma}^2 = RSE^2 = \frac{\sum_{i=1}^P(y_i - \hat{y}_i)^2}{P-2}.\]

\end{frame}


%%%%
\begin{frame}
    \frametitle{Confidence intervals, cont'd}
    How close do we suspect $w^\star$ and $b^\star$ to be to the \emph{true} (population) slope and intercept?
    
    Formulae: 
    \[(SE(w^\star))^2 = \frac{\sigma^2}{\sum_{i=1}^P(x_i - \bar{x})^2};\]
    \[(SE(b^\star))^2 = \sigma^2\left(\frac1{P} + \frac{\bar{x}^2}{\sum_{i=1}^P(x_i - \bar{x})^2}\right).\]
    
    Estimate:
        \[\sigma^2 \approx RSE^2 = \frac{\sum_{i=1}^P(y_i - \hat{y}_i)^2}{P-2}.\]
    
        \pause
    Can get (roughly) 95\% confidence interval\footnote{95\% of the time, these intervals contain population $w$, $b$.} with $\pm 2SE$: 
        \[(w^\star - 2SE(w^\star), w^\star + 2SE(w^\star))\]
    and 
        \[(b^\star - 2SE(b^\star), b^\star + 2SE(b^\star)).\]
    
\end{frame}

\section{Measuring how well LSR line fits}

%%%%
\begin{frame}
\frametitle{Mean Squared Error}
How to measure how well the data fits to regression line?

\pause 
In linear regression, we found ``predicted'' $\hat{y}_i = b^\star + {\bf x}_i^T{\bf w}^\star, 1\le i\le P$ so that the points $({\bf x}_1,\hat{y}_1), \ldots, ({\bf x}_P, \hat{y}_P)$ fit a line. Could use the Mean Squared Error as our measure.
    \[ \text{MSE} = \frac{1}{P}\sum_{i=1}^P (\hat{y}_i-y_i)^2. \]
\pause
\begin{itemize}
    \item For the same sample size, the larger the MSE is the farther $y_i$ is from $\hat{y}_i$, on average.
\end{itemize}

\pause
Closely related to RSE (residual standard error). Recall, 
    \[\textsf{RSE} = \sqrt{\frac{1}{P-2}\sum_{i=1}^P(y_i - \hat{y}_i)^2}.\]
So $\textsf{MSE} = \frac{P-2}{P}\textsf{RSE}^2$.
\end{frame}

%%%%
\begin{frame}
    \frametitle{Mean Squared Error, example}
    Recall, \ct{'Example1.csv'} data. Its best fit line is 
        \[y = 1.520275x - 0.33458.\]
    \onslide<2->{
    The MSE for this data and its predictions is $\approx 0.0197$. 

    Does that mean that the linear model is a ``good fit''?
    }

    \vfill
    \centering
    \includegraphics[height=0.35\textheight]{../../Images/example1-lsrline.png}
\end{frame}

%%%%
\begin{frame}
    \frametitle{Mean Squared Error, scaling}
    What about the following data and its best fit line? Here, the MSE is 1.9746.
    
    \begin{center}
    \includegraphics[height=0.35\textheight]{../../Images/example1-lsr-scaled.png}    
    \end{center}
    
    Is it still a good fit? 

    \pause
    The data here is from \ct{'Example1.csv'} again, except that the $y$-coordinates have been multiplied by 10. Its regression line is 
        \[y = 15.20275x - 3.3458.\]
    
    \pause 
    MSE is still a good measure to think about, but its size depends on scale of $y$-coordinates (equivalently, depends on units $y$ is measured in).

\end{frame}

%%%%
\begin{frame}
    \frametitle{$R^2$: Proportion of ``variance explained''}
    Get a measure that is unchanged by scaling: first, set \textbf{total sum of squares} (TSS) to 
    \[\textsf{TSS} = \sum_{i=1}^P(y_i - \bar{y})^2,\]
    where $\bar{y} = \frac{1}{P}\sum_{i=1}^P y_i$.

    \pause 
    Then, 
        \[R^2 = \frac{\textsf{TSS} - P\textsf{MSE}}{\textsf{TSS}} = 1 - \frac{\sum_{i=1}^P(y_i - \hat{y}_i)^2}{\sum_{i=1}^P(y_i - \bar{y}_i)^2}.\]
    \pause
    \begin{itemize}
        \item $R^2$ does \emph{not} depend on the scale of the $y$-coordinates.
        \pause
        \item Any data set, have $0\le R^2\le 1$ (provided $R^2$ is defined; i.e., we do not have $y_1,y_2,\ldots,y_n$ all the same).
        \begin{itemize}
            \item Can you prove this?
        \end{itemize}
        \pause
        \item Checking that $R^2$ is ``close'' to $1$ is often done to indicate that a linear model is a very good one.
    \end{itemize}
\end{frame}

\section{Example with Multiple variables}

%%%%
\begin{frame}
\frametitle{Linear regression, significance of variables}
{\color{mygreen}Checking significance of variables, an example:} Recall the \ct{'Advertising.csv'} data set, found on the class GitHub site, in folder DataSets.

\pause
\begin{itemize}
    \item Model \ttt{Sales} ($y$) as a function of advertising budgets in \ttt{TV} ($x_1$), \ttt{Radio} ($x_2$) and \ttt{Newspaper} ($x_3$).
    \pause
    %\item Fitting \ttt{Sales} to each one with single-variable regression (one for \ttt{TV}, one for \ttt{Radio}, one for \ttt{Newspaper}) is inadequate. 
    \pause
    \item Regression with just one of the variables ignores that all are contributing to \ttt{Sales} and doesn't predict $y$ very well.
\end{itemize}
\centering 
\includegraphics[width=0.5\textwidth]{../../Images/advertising-plot1.png}

\end{frame}

%%%%
\begin{frame}
\frametitle{Working with multiple independent variables}
    Rather than separate single-variable linear regressions, use multivariable linear regression. (We did this before with this example.)
\pause

    With ${\bf x} = (x_1,x_2,x_3)$ as the variables, use the model 
    \[y \approx b + {\bf x}^T{\bf w} = b + w_1x_1 + w_2x_2 + w_3x_3.\]

    \pause 
    As we discussed in Lecture 5, set $A$ to be $P\times(N+1)$ matrix with a column of ones, and a column for each independent variable (in this example, $N=3$). So, 
        \[A = \left[\vec{1}, \quad{\bf x}_1, \quad{\bf x}_2, \quad{\bf x}_3\right].\]
    $(b^\star, w_1^\star, w_2^\star, w_3^\star)$ is the solution to normal equations: $(A^TA)^{-1}(A^T{\bf y})$.

    \emph{General note}: the matrix $A^TA$ is invertible when $A$ has rank $N+1$ (when $\vec{1}, {\bf x}_1, \ldots, {\bf x}_N$ are linearly independent).\footnote{When a ``real world'' data set with $P\ge N+1$, almost surely.} 

    \pause
    \begin{itemize}
        \item Larger $N$ $\to$ more likely there are numerical issues computing inverse of $A^TA$.
    \end{itemize}
\end{frame}

%%%%
\begin{frame}
    \frametitle{Back to the example}
    $x_1$ for \ttt{TV} budget; \quad $x_2$ for \ttt{Radio} budget; \quad $x_3$ for \ttt{Newspaper} budget.
    
    \pause
    Multiple linear regression model for Advertising data is approximately 
        \[\hat{y} = 2.9389 + 0.0458x_1 + 0.1885x_2 - 0.001x_3.\]
    \pause
    Interpretation: given fixed budget for radio and newspaper ads, increasing TV ad budget by \$1000 will increase sales by around 46 units (in each market, on average).

    \centering
    \includegraphics[width=0.4\textheight]{../../Images/multiregression-higherangle.png}
\end{frame}

%%%%
\begin{frame}
\frametitle{$R^2$}
Here we consider the $R^2$ value from different choices for how many variables are used: one variable, two of the variables, or all 3 variables.

\pause
First, the result from single-variable regression:
\begin{center}
    \begin{tabular}{l c c c}
        Independent var. & \ttt{TV} & \ttt{Radio} & \ttt{Newspaper} \\ 
        \hline 
        $R^2$       &  0.612  &  0.332  &  0.052
    \end{tabular}
\end{center}

\pause
Now, $R^2$ for all possible pairs of two:
\begin{center}
    \begin{tabular}{l c c c}
        Two vars. & \ttt{TV}, \ttt{Radio} & \ttt{TV}, \ttt{Newspaper} & \ttt{Radio}, \ttt{Newspaper} \\ 
        \hline 
        $R^2$       &  0.89719  &  0.646  &  0.333
    \end{tabular}
\end{center}

\pause
The value of $R^2$ with all three predictor (independent) variables is: 0.89721. {\color{strings}What conclusion can we draw?}

\end{frame}

%%%%
\begin{frame}
    \frametitle{How small a change is `not significant'?}
    
    \pause 
    Hypothesis testing: choose a $p$-value threshold (often $<0.05$ or $<0.01$). The $p$-value corresponds to some $t$-statistic {--} use regression coefficient ($w_i^\star$ for $x_i$) and Standard Error.
    \begin{itemize}
        \item In example, if using simple linear regression on \ttt{Newspaper}, would get the variable is significant. However, using multiple regression with \ttt{TV}, \ttt{Radio}, and \ttt{Newspaper}, get very large $p$-value $\to$ so, not significant.
    \end{itemize}
    
    \pause 
    The formula for $\text{SE}(w_i)$ \ldots

    \pause 
    Alternative: if sample $w_i^\star$ varies a lot (relative to its size) compared to coeff's of the other var's, that variable is not significant.  
    \pause
    \begin{itemize}
        \item $p$-value large when $t$-statistic is small, which is when $SE(w_i^\star)$ is large \emph{relative to size of }$w_i^\star$.\qquad\footnote{Recall, on average, $SE$ is how far $w_i^\star$ is from population coeff.\ $w_i$.}
    \end{itemize}
\end{frame}

%%%%
\begin{frame}
    \frametitle{Intuitive estimate of significance (Bootstrapping)}
    Checking whether fluctuation of regression coefficient for an independent variable, relative to coeff.'s size, is large.

    \begin{enumerate}
        \pause
        \item Take around 100 random subsamples\footnote{$^\ast$Some evidence in literature (Goodhue-Lewis, 2012) that not much precision is to be gained with more than 100 samples, for bootstrapping standard errors.} of data (or, resamplings with replacement); compute coefficient $w_i^\star$ for those. Standard deviation of those sample coefficients $\approx$ $SE(w_i^\star)$.
        \pause
        \item Use regression coeff.\ from whole data set, $\approx w_i$. If standard dev.\ found in {\color{strings}1.}, divided by this coeff., is larger than about 0.5, variable is not significant.
        \begin{itemize}
            \item Since we are \emph{estimating some things} here, don't use as a hard cutoff. Getting 0.48, versus 0.59, would perhaps both be \emph{weakly} significant. However, if larger than 1.5, say, definitely not significant.
        \end{itemize}
    \end{enumerate}
    
    \footnote{This is an example of a bootstrapping procedure: the whole sample is used as a proxy for the population and the subsamples, or resamplings, are simulating samples from the population.}
\end{frame}

\section{Polynomial fitting}

%%%%
\begin{frame}
    \frametitle{Powers of $x$ in place of multiple variables}
    If a linear model does not seem a good fit for our data, can try fitting to a polynomial. This is just like doing linear regression on transformed data, {\bf except} with more than one function transformation: using $x$, and $x^2$, and $x^3$, etc.

    \pause
    As a regression model, we use 
        \[y \approx w_dx^d + w_{d-1}x^{d-1} + \ldots + w_1x + b\]
    for some degree $d$, then find the coefficients for a best fit polynomial.
    
    \pause
    Use essentially the same idea for the matrix $A$, but put powers of $x$ in each column (new features $= x^p$), instead of different independent variables. Given data $\{(x_i,y_i)\}_{i=1}^P$, the matrix $A$ is known as a \textbf{Vandermonde matrix}.
    \pause
    
    {\footnotesize
    \[A = \begin{bmatrix}x_1^d & \ldots & x_1^2 & x_1 & 1 \\ x_2^d & \ldots & x_2^2 & x_2 & 1 \\ 
        \vdots & & \vdots & \vdots \\
        x_P^d & \ldots & x_P^2 & x_P & 1 \\ \end{bmatrix}\]
    }
\end{frame}

%%%%
\begin{frame}
\frametitle{Example}
Taking the \lstinline[language=Python, stringstyle=\ttfamily\color{strings}]{'College.csv'} data set from the DataSets folder. Two of the columns are \lstinline[language=Python, stringstyle=\ttfamily\color{strings}]{'Top10perc'} and \lstinline[language=Python, stringstyle=\ttfamily\color{strings}]{'Top25perc'}. For the schools in the data set, these columns give the percentage of the entering class that were in the top 10\% (resp.\ 25\%) of their graduating high school class.\footnote{Removed rows that contained schools receiving fewer than 2500 applications.}
    

\pause
Here is the data set with a least squares line. The value of $R^2$ is 0.791. 
\begin{center}
    \includegraphics[height=0.35\textheight]{../../Images/CollegeTop25ontoTop10_deg1.png}
\end{center}

\end{frame}

%%%%
\begin{frame}
    \frametitle{Example}
    Here is the data set with a least squares line. The value of $R^2$ is 0.791. 
\begin{center}
    \includegraphics[height=0.35\textheight]{../../Images/CollegeTop25ontoTop10_deg1.png}
\end{center}

    \pause 
    Next, the data set with a least squares quadratic polynomial fit. The $R^2$ value is 0.854.

\begin{center}
    \includegraphics[height=0.35\textheight]{../../Images/CollegeTop25ontoTop10_deg2.png}
\end{center}
\end{frame}

%%%%
\begin{frame}
    \frametitle{Value of $R^2$ as polynomial degree increases}
    What will happen to the value of $R^2$ if we increase the degree of the polynomial that we fit to the data?

    \pause
    \begin{itemize}
        \item Note: Suppose that $P>d$. A Vandermonde matrix for $x$-values $x_1,x_2,\ldots, x_P$, which has $d+1$ columns (so, highest power is $x_i^d$), will have rank $d+1$ if and only if there are $d+1$ of the $x_i$ that are distinct.
        \pause
        \begin{itemize}
            \item[] \textit{If $x_1,x_2,\ldots,x_{d+1}$ are pairwise distinct, say, then the determinant of the $(d+1)\times(d+1)$ submatrix for their corresponding rows is}
            {\footnotesize
            \[{\displaystyle\prod_{1\le i<j\le d+1}(x_j - x_i)}.\]
            }
        \end{itemize}
    \end{itemize}

    \pause
    set $A_0$: the Vandermonde matrix used to fit polynomial of degree $d$; set $A_1$: the one used for polynomial of degree $d+1$. \footnote{So, $A_{1}$ has all the columns of $A_0$, and one additional column.} 
    
    From Note, as long as enough of the $x_i$ are distinct, $\operatorname{rank}(A_1) = \operatorname{rank}(A_0)+1$.

    \pause
    Meaning: $\textsf{Col}(A_0)$ is proper subspace of $\textsf{Col}(A_1)$. So, using $A_1$ makes $|y - \hat{y}|^2$ smaller. Since $\sum(y - \bar{y})^2$ is unchanged, makes $R^2$ closer to 1.
\end{frame}

\end{document}